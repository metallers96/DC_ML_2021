{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5997a9ab",
   "metadata": {},
   "source": [
    "#SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd561d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6e6ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15736b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris, y_iris = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88081353",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris,\n",
    "    y_iris,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e8f892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_iris_scaled = scaler.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler.transform(X_test_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05984c",
   "metadata": {},
   "source": [
    "# SVM linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53d0f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba28c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 94.167%\n",
      "test accuracy = 96.667%\n"
     ]
    }
   ],
   "source": [
    "lin_svm = LinearSVC(C=1, max_iter=10000).fit(X_train_iris_scaled, y_train_iris)\n",
    "print(f'train accuracy = {lin_svm.score(X_train_iris_scaled, y_train_iris):.3%}')\n",
    "print(f'test accuracy = {lin_svm.score(X_test_iris_scaled, y_test_iris):.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab71d9",
   "metadata": {},
   "source": [
    "#SVC RBF Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7eef395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9bc39da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 91.667%\n",
      "test accuracy = 83.333%\n"
     ]
    }
   ],
   "source": [
    "rbf_svm = SVC(C=10, kernel='rbf', gamma=0.001).fit(X_train_iris_scaled, y_train_iris)\n",
    "print(f'train accuracy = {rbf_svm.score(X_train_iris_scaled, y_train_iris):.3%}')\n",
    "print(f'test accuracy = {rbf_svm.score(X_test_iris_scaled, y_test_iris):.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f757ad3",
   "metadata": {},
   "source": [
    "#SVC Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ba92927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 100.000%\n",
      "test accuracy = 96.667%\n"
     ]
    }
   ],
   "source": [
    "poly_svm = SVC(C=5000, kernel='poly', degree=3).fit(X_train_iris_scaled, y_train_iris)\n",
    "print(f'train accuracy = {poly_svm.score(X_train_iris_scaled, y_train_iris):.3%}')\n",
    "print(f'test accuracy = {poly_svm.score(X_test_iris_scaled, y_test_iris):.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcdd8a",
   "metadata": {},
   "source": [
    "# Spam/Non-spam classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec381387",
   "metadata": {},
   "source": [
    "#Reveiw sample mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8252940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "cwd = os.getcwd() # current working directory\n",
    "path = os.path.join(cwd, 'data')\n",
    "fn = os.path.join(path , 'emailSample1.txt')\n",
    "content = get_sample(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b621c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323958d8",
   "metadata": {},
   "source": [
    "# Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d149469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbdd8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(content):\n",
    "    '''\n",
    "    content: str - body of mail \n",
    "    return: list of tokens (str) e.g. ['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a']\n",
    "    '''\n",
    "    # YOUR_CODE.  Split the content to tokens. You may need re.split()\n",
    "    # START_CODE \n",
    "#     tokens = np.array(re.split('[ \\n^(,)]', content))\n",
    "    tokens = np.array(re.split('[ \\n^]', content))\n",
    "    # END_CODE \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28d90f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
       "       'a', 'web', 'portal', '?', '>', 'Well,', 'it', 'depends', 'on',\n",
       "       'how', 'many', 'visitors', \"you're\", 'expecting.', 'This', 'can',\n",
       "       'be', 'anywhere', 'from', 'less', 'than', '10', 'bucks', 'a',\n",
       "       'month', 'to', 'a', 'couple', 'of', '$100.', '', 'You', 'should',\n",
       "       'checkout', 'http://www.rackspace.com/', 'or', 'perhaps', 'Amazon',\n",
       "       'EC2', '', 'if', 'youre', 'running', 'something', 'big..', '',\n",
       "       'To', 'unsubscribe', 'yourself', 'from', 'this', 'mailing',\n",
       "       'list,', 'send', 'an', 'email', 'to:',\n",
       "       'groupname-unsubscribe@egroups.com', '', ''], dtype='<U33')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(content)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69cad49",
   "metadata": {},
   "source": [
    "# Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c760eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of tokens in lower case (str)\n",
    "    '''\n",
    "    # YOUR_CODE.  Make all tokens in lower case\n",
    "    # START_CODE \n",
    "    tokens = np.char.lower(tokens)\n",
    "    # END_CODE \n",
    "   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc1df691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['>', 'anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
       "       'a', 'web', 'portal', '?', '>', 'well,', 'it', 'depends', 'on',\n",
       "       'how', 'many', 'visitors', \"you're\", 'expecting.', 'this', 'can',\n",
       "       'be', 'anywhere', 'from', 'less', 'than', '10', 'bucks', 'a',\n",
       "       'month', 'to', 'a', 'couple', 'of', '$100.', '', 'you', 'should',\n",
       "       'checkout', 'http://www.rackspace.com/', 'or', 'perhaps', 'amazon',\n",
       "       'ec2', '', 'if', 'youre', 'running', 'something', 'big..', '',\n",
       "       'to', 'unsubscribe', 'yourself', 'from', 'this', 'mailing',\n",
       "       'list,', 'send', 'an', 'email', 'to:',\n",
       "       'groupname-unsubscribe@egroups.com', '', ''], dtype='<U33')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = lower_case(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc795c",
   "metadata": {},
   "source": [
    "# Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54505358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens (tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of tokens replaced with corresponding unified words\n",
    "    '''\n",
    "    # YOUR_CODE.\n",
    "    #  You may  need re.sub()\n",
    "    # START_CODE \n",
    "    patterns = [\n",
    "        ('<[^<]+?>', ''), # Remove html and other tags\n",
    "        ('[0-9]{1,100}', 'number'), # mark all numbers \"number\"\n",
    "        ('^https?:\\/\\/(.*)', 'httpaddr'), # mark all  urls as \"httpaddr\"\n",
    "        ('[a-z0-9+._-]+@[a-z0-9._-]+\\.[a-z0-9_-]+', 'emailaddr'), # mark all emails as \"emailaddr\"\n",
    "        ('[$]', 'dollar'), # replace $ as \"dollar\"\n",
    "        ('[^\\w\\s]', '') # get rid of any punctuation | Remove any non alphanumeric characters\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        tokens = np.array([re.sub(pattern[0], pattern[1], string) for string in tokens])\n",
    "\n",
    "    # END_CODE\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f74d5456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
       "       'a', 'web', 'portal', '', '', 'well', 'it', 'depends', 'on', 'how',\n",
       "       'many', 'visitors', 'youre', 'expecting', 'this', 'can', 'be',\n",
       "       'anywhere', 'from', 'less', 'than', 'number', 'bucks', 'a',\n",
       "       'month', 'to', 'a', 'couple', 'of', 'dollarnumber', '', 'you',\n",
       "       'should', 'checkout', 'httpaddr', 'or', 'perhaps', 'amazon',\n",
       "       'ecnumber', '', 'if', 'youre', 'running', 'something', 'big', '',\n",
       "       'to', 'unsubscribe', 'yourself', 'from', 'this', 'mailing', 'list',\n",
       "       'send', 'an', 'email', 'to', 'emailaddr', '', ''], dtype='<U12')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = normalize_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f5f78",
   "metadata": {},
   "source": [
    "# Remove zero length tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a1bb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_tokens (tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of filtered tokens (str)\n",
    "    '''\n",
    "    original_tokens_len = len(tokens)\n",
    "    \n",
    "    # YOUR_CODE. Keep only tokens that lenght >0  \n",
    "    # START_CODE \n",
    "    indexes = np.where(tokens != '')\n",
    "    tokens = tokens[indexes]\n",
    "    # END_CODE     \n",
    "   \n",
    "    print (f'Original len = {original_tokens_len}\\nRemaining len = {len(tokens)}')    \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c98b757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len = 69\n",
      "Remaining len = 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a',\n",
       "       'web', 'portal', 'well', 'it', 'depends', 'on', 'how', 'many',\n",
       "       'visitors', 'youre', 'expecting', 'this', 'can', 'be', 'anywhere',\n",
       "       'from', 'less', 'than', 'number', 'bucks', 'a', 'month', 'to', 'a',\n",
       "       'couple', 'of', 'dollarnumber', 'you', 'should', 'checkout',\n",
       "       'httpaddr', 'or', 'perhaps', 'amazon', 'ecnumber', 'if', 'youre',\n",
       "       'running', 'something', 'big', 'to', 'unsubscribe', 'yourself',\n",
       "       'from', 'this', 'mailing', 'list', 'send', 'an', 'email', 'to',\n",
       "       'emailaddr'], dtype='<U12')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = filter_short_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38cb48",
   "metadata": {},
   "source": [
    "# Stem tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff33d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c78d9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of stemmed tokens e.g. array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani']...\n",
    "    '''\n",
    "    # YOUR_CODE. replace the tokens by stemmed fortokens by stemmed formm. You may need PorterStemmer.stem() \n",
    "    # START_CODE \n",
    "    porter = PorterStemmer()\n",
    "    tokens = np.array([porter.stem(word) for word in tokens])\n",
    "    # END_CODE     \n",
    "   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5f3a6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
       "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani',\n",
       "       'visitor', 'your', 'expect', 'thi', 'can', 'be', 'anywher', 'from',\n",
       "       'less', 'than', 'number', 'buck', 'a', 'month', 'to', 'a', 'coupl',\n",
       "       'of', 'dollarnumb', 'you', 'should', 'checkout', 'httpaddr', 'or',\n",
       "       'perhap', 'amazon', 'ecnumb', 'if', 'your', 'run', 'someth', 'big',\n",
       "       'to', 'unsubscrib', 'yourself', 'from', 'thi', 'mail', 'list',\n",
       "       'send', 'an', 'email', 'to', 'emailaddr'], dtype='<U10')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = stem_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3724cfe",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54638d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab) = 1,899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocabulary(fn):\n",
    "    '''\n",
    "    fn: str - full path to file \n",
    "    return: ndarray of str e.g. array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)\n",
    "    '''\n",
    "    vocab_list = pd.read_table(fn, header=None)\n",
    "    vocab = np.array(vocab_list)[:,1] # first columns is index, select only words column  \n",
    "    print(f'len(vocab) = {len(vocab):,}')\n",
    "    return vocab\n",
    "\n",
    "fn = os.path.join(path , 'vocab.txt')\n",
    "vocab = get_vocabulary(fn)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733bee1",
   "metadata": {},
   "source": [
    "# Feature reresentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b77052fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_features(tokens, vocab):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of binary values 1 if word from vocabulary is in mail 0 otherwise\n",
    "    '''\n",
    "    # YOUR_CODE. Compute the array with 1/0 corresponding to is word from vocabulary in mail \n",
    "    # START_CODE \n",
    "    tokens_represented = np.array([1 if token in tokens else 0 for token in vocab.astype('str')])\n",
    "    # END_CODE     \n",
    "\n",
    "    print (f'{np.sum(tokens_represented)} word(s) from vocab are in the tokens.')\n",
    "\n",
    "    return tokens_represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61b3bdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 word(s) from vocab are in the tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_represented = represent_features(tokens, vocab)\n",
    "tokens_represented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d08e1",
   "metadata": {},
   "source": [
    "# Composing all steps of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f49de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess (content, vocab):\n",
    "    '''\n",
    "    content: str - body of mail \n",
    "    vocab: ndarray of str - list of considered words \n",
    "    '''\n",
    "    # YOUR_CODE. Compute the array with 1/0 corresponding to is word from vocabulary in mail \n",
    "    # START_CODE \n",
    "\n",
    "    # tokenize content    \n",
    "    tokens  = word_tokenize(content)\n",
    "    \n",
    "    # make lower case\n",
    "    tokens = lower_case(tokens)\n",
    "\n",
    "    # normalize tokens\n",
    "    tokens = normalize_tokens(tokens)\n",
    "\n",
    "    # remove zero words\n",
    "    tokens = filter_short_tokens(tokens)\n",
    "    \n",
    "    # stem words\n",
    "    tokens = stem_tokens(tokens)\n",
    "    \n",
    "    # convert to binary array of features  \n",
    "    tokens_represented = represent_features(tokens, vocab)\n",
    "    # END_CODE     \n",
    "    \n",
    "    return tokens_represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84d88704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len = 69\n",
      "Remaining len = 61\n",
      "44 word(s) from vocab are in the tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(content, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4273a",
   "metadata": {},
   "source": [
    "# Training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "066866f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f8620ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (4000, 1899)\n",
      "y_train.shape = (4000,)\n",
      "X_test.shape = (1000, 1899)\n",
      "y_test.shape = (1000,)\n",
      "Sample with index = 0: \n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "fn = os.path.join(path , 'spamTrain.mat')\n",
    "\n",
    "mat = loadmat(fn)\n",
    "X_train = mat['X']\n",
    "y_train = mat['y'].ravel()\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'y_train.shape = {y_train.shape}')\n",
    "\n",
    "fn = os.path.join(path, 'spamTest.mat')\n",
    "mat = loadmat(fn)\n",
    "X_test = mat['Xtest']\n",
    "y_test = mat['ytest'].ravel() \n",
    "\n",
    "print (f'X_test.shape = {X_test.shape}')\n",
    "print (f'y_test.shape = {y_test.shape}')\n",
    "index = 0 \n",
    "print (f'Sample with index = {index}: \\n{X_train[index]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8468f",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b33acf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train = 0.99975\n",
      "Score test = 0.992\n"
     ]
    }
   ],
   "source": [
    "C = .1\n",
    "clf = LinearSVC(C=C)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f'Score train = {clf.score(X_train,y_train)}')\n",
    "print(f'Score test = {clf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a74a7b",
   "metadata": {},
   "source": [
    "# Determining most spam contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7e45412",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_coef = pd.Series(clf.coef_[0], index=vocab, name='Coef')\n",
    "top_spam_contributors = ser_coef.sort_values(ascending=False).index[:20].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab95bf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'remov', 'click', 'basenumb', 'guarante', 'visit', 'bodi', 'will', 'numberb', 'price', 'dollar', 'nbsp', 'below', 'lo', 'most', 'send', 'dollarnumb', 'credit', 'wi', 'hour']\n"
     ]
    }
   ],
   "source": [
    "print(top_spam_contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b059be",
   "metadata": {},
   "source": [
    "# Use model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63699dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len = 69\n",
      "Remaining len = 61\n",
      "44 word(s) from vocab are in the tokens.\n",
      "emailSample1.txt is Not Spam\n",
      "\n",
      "Original len = 247\n",
      "Remaining len = 222\n",
      "122 word(s) from vocab are in the tokens.\n",
      "emailSample2.txt is Not Spam\n",
      "\n",
      "Original len = 141\n",
      "Remaining len = 97\n",
      "46 word(s) from vocab are in the tokens.\n",
      "spamSample1.txt is Spam\n",
      "\n",
      "Original len = 39\n",
      "Remaining len = 31\n",
      "18 word(s) from vocab are in the tokens.\n",
      "spamSample2.txt is Spam\n",
      "\n",
      "Latter sample:\n",
      "==================================================\n",
      "Best Buy Viagra Generic Online\n",
      "\n",
      "Viagra 100mg x 60 Pills $125, Free Pills & Reorder Discount, Top Selling 100% Quality & Satisfaction guaranteed!\n",
      "\n",
      "We accept VISA, Master & E-Check Payments, 90000+ Satisfied Customers!\n",
      "http://medphysitcstech.ru\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for sfn in [ 'emailSample1.txt', 'emailSample2.txt', 'spamSample1.txt', 'spamSample2.txt']:\n",
    "    fn =  os.path.join(path,sfn)    \n",
    "    content = get_sample(fn)\n",
    "    \n",
    "    # YOUR_CODE.  Preprocess the sample and get prediction 0 or 1 (1 is spam)\n",
    "    # START_CODE\n",
    "    prediction = clf.predict([preprocess(content, vocab)])\n",
    "    # END_CODE    \n",
    "    \n",
    "    print ('{} is {}\\n'.format(sfn, ('Not Spam','Spam')[prediction[0]]))\n",
    "\n",
    "print ('Latter sample:\\n{1}\\n{0}\\n{1}'.format(content, '='*50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaea68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65781e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
